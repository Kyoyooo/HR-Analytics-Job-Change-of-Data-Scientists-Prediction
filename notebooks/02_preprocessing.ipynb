{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1ab8496-6519-487d-b7ec-85ce67cc1ef1",
   "metadata": {},
   "source": [
    "# HW2: Numpy cho Khoa học dữ liệu - Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16e8c0-8b73-45ed-ae46-4d74047fe0a8",
   "metadata": {},
   "source": [
    "Quy trình tiền xử lý dữ liệu này được thể hiện thông qua các bước như sau: \n",
    "- Đọc và load dữ liệu\n",
    "- Kiểm tra tính hợp lệ của dữ liệu \n",
    "- Xử lý missing values\n",
    "- Tính toán thống kê mô tả, kiểm định giả thiết thống kê\n",
    "- Feature engineering\n",
    "- Xác định và loại bỏ (nếu cần thật sự cần thiết) đối với các giá trị ngoại lai\n",
    "- Chuẩn hoá (Normalization) cho từng đặc trưng\n",
    "- Điều chuẩn khoảng giá trị phù hợp dữ liệu Non-Gaussian Distribution\n",
    "- Điều chuẩn dữ liệu (Standardization) hay Z-score để đạt trung bình 0 và phương sai 1 trước khi sử dụng thuật toán dựa trên gradient hoặc feature engineering bằng các kỹ thuật dimensional reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6f897-3a7d-40ec-b09e-a5ed13e7f588",
   "metadata": {},
   "source": [
    "Trước tiên tiến hành import thư viện `Numpy` cho tiền xử lý, `sys` và `os` hỗ trợ việc đọc và load dữ liệu, các hàm xử lý các yêu cầu về tiền xử lý dữ liệu từ file `data_processing.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb06436-6560-435e-83df-d55f4518f5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Thêm src vào path để import được\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "from data_processing import (\n",
    "    load_csv_numpy, fill_missing_categorical, ordinal_encode_experience, label_encode,\n",
    "    remove_outliers_iqr, create_new_features, validate_data_integrity, min_max_normalize, \n",
    "    log_transformation, decimal_scaling, standard_scale, robust_scale, \n",
    "    calculate_t_test_ind, save_processed_csv, perform_pca\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d24df-7fca-4528-b7fb-a0dfe173b7ca",
   "metadata": {},
   "source": [
    "## Đọc và load dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239ef21a-1611-4bf4-8ddd-f6a86735310a",
   "metadata": {},
   "source": [
    "Từ các dữ liệu đã được chuẩn bị sẵn trong thư mục `data/raw`, tiến hành đọc và load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01767dbb-1dcb-414f-aaa0-02f655cf508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('..', 'data', 'raw', 'aug_train.csv')\n",
    "test_path = os.path.join('..', 'data', 'raw', 'aug_test.csv')\n",
    "\n",
    "# Khai báo kiểu dữ liệu cho từng đặc trưng\n",
    "types_train = [\n",
    "    'i8',   # enrollee_id\n",
    "    'U50',  # city\n",
    "    'f8',   # city_development_index\n",
    "    'U50',  # gender\n",
    "    'U50',  # relevent_experience\n",
    "    'U50',  # enrolled_university\n",
    "    'U50',  # education_level\n",
    "    'U50',  # major_discipline\n",
    "    'U50',  # experience\n",
    "    'U50',  # company_size\n",
    "    'U50',  # company_type\n",
    "    'U50',  # last_new_job\n",
    "    'i8',   # training_hours\n",
    "    'f8'    # target\n",
    "]\n",
    "\n",
    "# Bỏ đặc trưng cuối cho tập test\n",
    "types_test = types_train[:-1]\n",
    "\n",
    "header_train, data_train = load_csv_numpy(train_path, types_train)\n",
    "header_test, data_test = load_csv_numpy(test_path, types_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe03f0-8fa0-4c60-b54e-22f8101673bb",
   "metadata": {},
   "source": [
    "## Kiểm tra tính hợp lệ của dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50a45f-a82d-4eca-89ea-2761ae8f1487",
   "metadata": {},
   "source": [
    "Bước đầu của việc tiền xử lý dữ liệu là kiểm tra tính hợp lệ, ở đây có 2 điểm đáng chú ý cần được xem xét:\n",
    "- Kiểm tra thuộc tính \"training_hours\" có dương hay không\n",
    "- Kiểm tra thuộc tính \"city_development_index\" có nằm trong khoảng 0 - 1 hay không "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dd552f-f878-4ed0-90b6-cb910d33d4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu sạch, không có giá trị vô lý\n"
     ]
    }
   ],
   "source": [
    "validate_data_integrity(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560e3f5-021b-4ca1-9b64-6710285900a9",
   "metadata": {},
   "source": [
    "## Xử lý missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e600757-90b0-468a-8db5-bd70644d3891",
   "metadata": {},
   "source": [
    "Ý tưởng xử lý missing values được chia ra 2 hướng chính: \n",
    "- Điền vào giá trị phổ biến nhất cho các vị trí thiếu\n",
    "- Gán trực tiếp giá trị \"Unknown\" vào các vị trí thiếu\n",
    "\n",
    "Ở đây, thuộc tính `gender` và `major_discipline` được áp dụng cách 2 vì việc một ứng viên không điền giới tính hoặc chuyên ngành có thể mang mục đích cá nhân (ví dụ như muốn giấu thông tin, hoặc học chuyên ngành lạ không có trong danh sách). Vì vậy, việc gán giá trị \"Unknown\" giúp nó trở thành một thông tin để  xây dựng mô hình tốt hơn, bởi lẽ nếu điền vào giá trị phổ biến nhất (ví dụ điền hết là \"Male\" chẳng hạn) thì vô tình sẽ làm nhiễu dữ liệu (dữ liệu mang tính biased lớn).\n",
    "\n",
    "Các thuộc tính còn lại thường bị mất dữ liệu chủ yếu do yếu tố  ngẫu nhiên hoặc do trong quá trình thu thập dữ liệu người được khảo sát lười điền vào chứ không phải do có những lựa chọn khác ngoài lề. Bởi vậy, ưu tiên điền vào giá trị phổ biến nhất để  có thể duy trì phân phối tổng thể của dữ liệu, tránh tạo ra quá nhiều giá trị \"Unknown\" gây loãng dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0364e3fb-dbe3-4ac5-b746-a0daf2cc3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_pipeline(data):\n",
    "    data = fill_missing_categorical(data, 'gender', strategy='unknown') \n",
    "    data = fill_missing_categorical(data, 'major_discipline', strategy='unknown')\n",
    "    \n",
    "    cat_cols = ['enrolled_university', 'company_size', 'company_type', \n",
    "                'education_level', 'last_new_job']\n",
    "    for col in cat_cols:\n",
    "        data = fill_missing_categorical(data, col, strategy='mode')\n",
    "        \n",
    "    data = fill_missing_categorical(data, 'experience', strategy='mode')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfbf709-a5ca-4cfa-aed0-5c086a3d858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fill_missing_pipeline(data_train)\n",
    "data_test = fill_missing_pipeline(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bfc3c2-0875-47c5-8fe3-40c6af337b2e",
   "metadata": {},
   "source": [
    "## Kiểm định giả thuyết thống kê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8275e12-f2c8-40b3-935e-1fbef434b511",
   "metadata": {},
   "source": [
    "Ở đây sẽ tập trung kiểm định T-test độc lập để so sánh giá trị trung bình của đặc trưng `training_hours` giữa hai nhóm ứng viên: nhóm không đổi việc (Target=0) và nhóm muốn đổi việc (Target=1) với mục đích là để trả lời cho câu hỏi: \"Việc ứng viên tham gia đào tạo nhiều giờ hơn có liên quan đến việc họ muốn bỏ việc hay không?\". Ý tưởng đặt ra như sau: \n",
    "- Đặt giả thiết (Hypothesis) cho câu hỏi:\n",
    "  + Giả thiết H0 (Null Hypothesis): $\\mu_0 = \\mu_1$ (Thời gian đào tạo trung bình của hai nhóm là như nhau).\n",
    "  + Giả thiết H1 (Alternative Hypothesis): $\\mu_0 \\neq \\mu_1$ (Có sự khác biệt về thời gian đào tạo).\n",
    "- Lấy ra 2 mẫu dữ liệu của 2 thuộc tính `training_hours` và `target`, tính phương sai mẫu cho 2 mẫu dữ liệu này và sử dụng loại kiểm định T-test cho 2 phương sai không bằng nhau, với công thức sai số chuẩn (Standard Error - SE) được dùng là: $SE = \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}$.\n",
    "- Ngưỡng đánh giá (Critical Value): Xét với độ tin cậy 95% (mức ý nghĩa $\\alpha=0.05$), từ đó đưa ra kết luận. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9a95649-abfc-4ab7-9e75-149570f4c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bác bỏ H0: Ứng viên tham gia đào tạo nhiều giờ hơn có liên quan đến việc họ muốn bỏ việc\n",
      "Người học ít hơn có xu hướng nhảy việc cao hơn\n"
     ]
    }
   ],
   "source": [
    "hours = data_train['training_hours']\n",
    "target = data_train['target']\n",
    "\n",
    "group_0 = hours[target == 0.0]\n",
    "group_1 = hours[target == 1.0]\n",
    "\n",
    "t_stat, mean0, mean1 = calculate_t_test_ind(group_0, group_1)\n",
    "\n",
    "if abs(t_stat) > 1.96: # Mức ý nghĩa 5%\n",
    "    print(\"Bác bỏ H0: Ứng viên tham gia đào tạo nhiều giờ hơn có liên quan đến việc họ muốn bỏ việc\")\n",
    "    if mean1 > mean0:\n",
    "        print(\"Người học nhiều hơn có xu hướng nhảy việc cao hơn\")\n",
    "    elif mean1 < mean0:\n",
    "        print(\"Người học ít hơn có xu hướng nhảy việc cao hơn\")\n",
    "else:\n",
    "    print(\"Chấp nhận H0: Ứng viên tham gia đào tạo nhiều giờ hơn không liên quan đến việc họ muốn bỏ việc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2cea6f-7b5d-4e01-8917-e9fe365d8cda",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dc198b-e27c-4780-ab26-b73456183b5d",
   "metadata": {},
   "source": [
    "Đầu tiên là chuyển đổi dữ liệu dạng chuỗi sang dạng số, áp dụng cho các giá trị của thuộc tính `experience` để giải quyết dưới dạng chuỗi hỗn hợp (ví dụ như \"1\", \"5\", \"<1\", \">20\"). Ý tưởng xử lý như sau: \n",
    "- `\"<1\"` $\\rightarrow$ `0`: Quy ước chưa có kinh nghiệm là 0 năm.\n",
    "- `\">20\"` $\\rightarrow$ `21`: Quy ước trên 20 năm là 21 (để giữ tính thứ tự lớn nhất).\n",
    "- `\"\"` (Rỗng) $\\rightarrow$ `0`: Xử lý missing value (giả định thiếu là chưa có kinh nghiệm).\n",
    "- Với các giá trị còn lại (ví dụ \"5\", \"10\") thì ép kiểu sang float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f16d95f-f577-4c95-9e9a-b8675617234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train = ordinal_encode_experience(data_train)\n",
    "exp_test = ordinal_encode_experience(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d697e1-9162-4e51-9ad7-1871314a7b83",
   "metadata": {},
   "source": [
    "Tiếp theo là tạo ra 2 đặc trưng mới: \n",
    "- Đặc trưng 1: `Interaction` \n",
    "  + Công thức: `cdi_exp = cdi * exp_numeric`\n",
    "  + Ý nghĩa: Kết hợp Chỉ số phát triển thành phố và Kinh nghiệm. Ví dụ như một người có 20 năm kinh nghiệm (Exp = 21) sống ở thành phố rất phát triển (CDI = 0.9) sẽ có chỉ số này rất cao ($\\approx 18.9$). Điều này cho thấy người này thuộc nhóm chuyên gia cao cấp ổn định, hành vi nhảy việc sẽ khác với người mới ra trường ở vùng kém phát triển.\n",
    "- Đặc trưng 2: `Training Intensity` \n",
    "  + Công thức: `intensity = th / (exp_numeric + 1.0)`\n",
    "  + Ý nghĩa: Đo Cường độ học tập của ứng viên. Lấy ví dụ như người A học 100 giờ và có Kinh nghiệm 1 năm thì sẽ có Training Intensity cao hơn người B học 100 giờ và có kinh nghiệm 20 năm, cho thấy người A rất chăm chỉ học hỏi với kinh nghiệm còn ít ỏi của mình, còn người B thì mức độ học được đánh giá là bình thường so với thâm niên. Đặc trưng này giúp mô hình phân biệt được ai là người đang nỗ lực học để thăng tiến sự nghiệp nhanh chóng (thường sẽ có Training Intensity cao) so với những người chỉ học duy trì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecde30b0-f1c9-4226-88a5-50df3e30e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdi_exp_train, intensity_train = create_new_features(data_train, exp_train)\n",
    "cdi_exp_test, intensity_test = create_new_features(data_test, exp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a157f264-da83-42ab-982d-7682e9180059",
   "metadata": {},
   "source": [
    "Lấy ra các đặc trưng gốc dạng số để xử lý bước tiếp theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a05227-f4bc-441e-8d24-2512f33d27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_train = data_train['training_hours']\n",
    "cdi_train = data_train['city_development_index']\n",
    "th_test = data_test['training_hours']\n",
    "cdi_test = data_test['city_development_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6aa34-3833-4872-92b0-26bec69ae57b",
   "metadata": {},
   "source": [
    "## Xử lý các giá trị ngoại lai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa38588-fc00-45c6-8d4d-c7bdf963f4d5",
   "metadata": {},
   "source": [
    "Trọng tâm của phần này là sử dụng phương pháp IQR (Interquartile Range), nhưng trước tiên cần xử lý các đặc trưng bị lệch (skewed) là `training_hours` và `intensity` bằng cách áp dụng kỹ thuật `log transformation`. Lý do là nếu dùng IQR ngay trên dữ liệu thô, giá trị bị xô lệch lớn sẽ bị coi là ngoại lai và bị xóa, bởi thế mà cần dùng `log transformation` để khoảng cách giữa các giá trị được thu hẹp lại và phân phối trở nên đối xứng hơn. Lúc này, IQR sẽ hoạt động chính xác hơn, chỉ loại bỏ những giá trị nhiễu thực sự và giữ lại những giá trị cao hợp lý.\n",
    "\n",
    "Các kỹ thuật đáng chú ý cho log transformation nằm ở 2 chỗ:\n",
    "- **Shift (Dịch chuyển)**: `if np.min(array) < 0: array = array - np.min(array)` Bước này đảm bảo dữ liệu luôn không âm trước khi đưa vào hàm Log (vì log của số âm là không xác định).\n",
    "- **Log1p**: Sử dụng `np.log1p(array)` tương đương với $\\ln(1 + x)$. Việc cộng thêm 1 giúp xử lý trường hợp $x=0$ (vì $\\ln(0) = -\\infty$, gây lỗi tính toán).\n",
    "\n",
    "Tác dụng chính của `log transformation` là biến đổi phân phối có đặc điểm long tail thành dạng gần giống hình chuông (Gaussian) hơn. Điều này cực kỳ quan trọng vì các thuật toán máy học như Linear/Logistic Regression hoạt động tốt nhất trên phân phối chuẩn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67498813-fd81-4cb9-b38c-d605c544edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_train_log = log_transformation(th_train)\n",
    "th_test_log = log_transformation(th_test)\n",
    "\n",
    "intensity_train_log = log_transformation(intensity_train)\n",
    "intensity_test_log = log_transformation(intensity_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bbcf6-0ae4-407f-9656-2df63a77ca01",
   "metadata": {},
   "source": [
    "Ý tưởng chính cho việc xác định và loại bỏ các giá trị ngoại lại (hàm `remove_outliers_iqr`) như sau:\n",
    "- Áp dụng phương pháp IQR:\n",
    "  + Tính $Q1$ (25%) và $Q3$ (75%).\n",
    "  + Tính $IQR = Q3 - Q1$.\n",
    "  + Xác định biên: $[Q1 - 1.5 \\times IQR, Q3 + 1.5 \\times IQR]$. Bất cứ điểm dữ liệu nào nằm ngoài khoảng này được coi là ngoại lai.\n",
    "- Hàm trả về một mảng Boolean các giá trị True/False (mảng mask). Mảng mask này chỉ tính toán trên đặc trưng `training_hours` và cũng chính là biến `mask_train`, sau đó áp dụng mask này cho các đặc trưng khác như `cdi_train`, `exp_train`, `intensity_train`... Mục đích ở đây là nếu gặp trường hợp ứng viên A có giá trị của đặc trưng `training_hours` bị ngoại lai, toàn bộ thông tin của ứng viên A (tất cả các giá trị ở dòng của ứng viên A) sẽ bị xoá để đảm bảo tính toàn vẹn của dữ liệu. Lí do cho việc xoá đi toàn bộ là bởi nếu chỉ xóa `training_hours` mà giữ lại `target`, dòng dữ liệu đó sẽ bị lệch pha và gây lỗi khi xây dựng mô hình.\n",
    "\n",
    "Ở đây xử lý ngoại lai chỉ áp dụng cho tập dữ liệu train, không áp dụng cho tập dữ liệu test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d0a548e-8635-4a28-8e13-7243c41ab7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dụng IQR để lọc dựa trên training_hours\n",
    "mask_train = remove_outliers_iqr(th_train_log)\n",
    "\n",
    "# Áp dụng mask để lọc cho dữ liệu train (các đặc trưng thuộc loại định lượng)\n",
    "th_train_log = th_train_log[mask_train]\n",
    "cdi_train = cdi_train[mask_train]\n",
    "exp_train = exp_train[mask_train]\n",
    "cdi_exp_train = cdi_exp_train[mask_train]\n",
    "intensity_train = intensity_train[mask_train]\n",
    "y_train = data_train['target'][mask_train]\n",
    "\n",
    "# Lọc dữ liệu train (các đặc trưng thuộc loại định tính) để đồng bộ\n",
    "data_train_filtered = data_train[mask_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9d35e-c728-496e-b298-dd729b944157",
   "metadata": {},
   "source": [
    "## Chuẩn hoá (Normalization) cho từng đặc trưng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a0a4b-5458-4b90-9cd8-553bf86de584",
   "metadata": {},
   "source": [
    "Kỹ thuật `min_max_normalize` được áp dụng cho đặc trưng `city_development_index` với cơ chế hoạt động như sau: Co giãn dữ liệu về đúng một khoảng cố định, thường là trong đoạn [0, 1].\n",
    "\n",
    "Công thức: $$X_{new} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "Kỹ thuật này mang lại 2 lợi ích: \n",
    "- Các giá trị của đặc trưng `city_development_index` thông qua bộ dữ liệu cho thấy nó được giới hạn rõ ràng trong đoạn [0, 1]. Vì vậy nên áp dụng kỹ thuật này không phải lo việc bị đụng phải các giá trị ngoại lai vốn là hạn chế của `min_max_normalize`. \n",
    "- Kỹ thuật này giúp giữ nguyên khoảng cách tỷ lệ giữa các thành phố. Ví dụ như với thành phố A (0.8) và thành phố B (0.4), sau khi chuẩn hoá thì A vẫn gấp đôi B mà không làm xô lệch đi cấu trúc phân phối gốc của dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98fe6c22-76d3-43fc-95e3-c9a7622b4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdi_train_norm = min_max_normalize(cdi_train)\n",
    "cdi_test_norm = min_max_normalize(cdi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11506b53-2f31-45c7-bb3f-20ee9971992b",
   "metadata": {},
   "source": [
    "Kỹ thuật `decimal_scaling` được áp dụng cho đặc trưng `experience` với cơ chế hoạt động như sau: Di chuyển dấu thập phân của số liệu sang trái cho đến khi giá trị tuyệt đối nhỏ hơn 1.\n",
    "\n",
    "Công thức: $$X_{new} = \\frac{X}{10^j}$$ \n",
    "    (với $j$ là số nguyên nhỏ nhất để $|X_{new}| < 1$).\n",
    "\n",
    "Kỹ thuật này mang lại lợi ích:\n",
    "- Đặc thù của dữ liệu trong `experience` là số nguyên dương, thường nằm trong khoảng 0 đến 30 năm. Ở đây giá trị lớn nhất là khoảng hơn 20.\n",
    "- Với $j=2$ (chia cho 100), 5 năm trở thành 0.05, 20 năm trở thành 0.20. Với dữ liệu đã chuẩn hoá (0.20) này, chúng ta khi nhìn vào vẫn có thể hiểu đó là 20 năm. Kỹ thuật này giúp giảm độ lớn số học để thu hẹp đi chênh lệch với giá trị của các đặc trưng khác để xây dựng mô hình hiệu quả mà vẫn giữ nguyên ý nghĩa ban đầu của dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e1f5d0-a22c-43a3-aea0-4caf01fcf450",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_train_dec = decimal_scaling(exp_train)\n",
    "exp_test_dec = decimal_scaling(exp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d615c64-2e25-433b-9fdd-cebed719f47e",
   "metadata": {},
   "source": [
    "## Điều chuẩn khoảng giá trị phù hợp dữ liệu Non-Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30299540-f12b-4536-a190-ef5cb075525e",
   "metadata": {},
   "source": [
    "Sau khi dùng kỹ thuật `log transformation` thì kỹ thuật `Robust Scaling` cũng được áp dụng tiếp cho 2 đặc trưng `training_hours` và `intensity` với cơ chế hoạt động như sau: Sử dụng các đại lượng thống kê vị trí (Median, Quartile) thay vì đại lượng độ lớn (Mean, Std).\n",
    "\n",
    "Công thức: $$X_{new} = \\frac{X - Median}{IQR}$$ (với $IQR = Q3 - Q1$).\n",
    "\n",
    "Kỹ thuật này mang lại 1 số lợi ích:\n",
    "- `training_hours` thông thường có phân phối lệch (Non-Gaussian Distribution) và chứa ngoại lai, lấy ví dụ như đa số học 20h, nhưng lại có người học 300h. Nếu dùng kỹ thuật `standard_scale` để chuẩn hoá về độ lệch chuẩn thì có khả năng giá trị 300h sẽ kéo Mean lên cao và làm tăng độ lệch chuẩn (Std). Kết quả là các giá trị bình thường (20h) sẽ bị ép về một khoảng rất nhỏ quanh giá trị 0, làm mất thông tin chi tiết của đa số dữ liệu.\n",
    "- Thay vì như vậy, dùng Median và IQR là đặc trưng của `Robust Scaling`. Kỹ thuật này giải quyết tốt vấn đề dữ liệu Non-Gaussian Distribution hoặc chứa nhiều giá trị bất thường bằng cách dùng giá trị Median làm trung tâm của dữ liệu. Giá trị ngoại lai 300h không ảnh hưởng đến Median, do đó mà khoảng cách giữa các giá trị bình thường (10h, 20h, 30h) được bảo toàn tốt hơn, không bị kéo theo các giá trị ngoại lai vốn là thiểu số trong tập dữ liệu. Đây cũng là kỹ thuật giúp tránh đi nhiễu trong dữ liệu tốt nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b722ccb9-2430-4f9a-a73e-ab4adc0baf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "th_train_robust = robust_scale(th_train_log)\n",
    "th_test_robust = robust_scale(th_test_log)\n",
    "\n",
    "intensity_train_robust = robust_scale(intensity_train)\n",
    "intensity_test_robust = robust_scale(intensity_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4470227-9dac-43aa-8ddb-b7acf250171b",
   "metadata": {},
   "source": [
    "## Điều chuẩn dữ liệu (Standardization) hay Z-score để đạt trung bình 0 và phương sai 1 trước khi sử dụng thuật toán dựa trên gradient hoặc feature engineering bằng các kỹ thuật dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d815ce-fa7c-46d1-9278-fbaf8381de09",
   "metadata": {},
   "source": [
    "Đối với đặc trưng `cdi_exp` và các đặc trưng thuộc loại định tính, cần áp dụng kỹ thuật `standard_scale` để  điều chuẩn dữ liệu hay Z-score để đạt trung bình $\\mu$ = 0 và phương sai $\\sigma^2$ = 1 với cơ chế hoạt động đưa dữ liệu về phân phối chuẩn tắc.\n",
    "\n",
    "Công thức như sau: $$X_{new} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Có 1 số lý do để chọn kỹ thuật này cho các đặc trưng này:\n",
    "- `cdi_exp` vốn dĩ là thuộc tính được tạo thêm từ tích của hai thuộc tính có sẵn, bởi thế nên phân phối nên có xu hướng quy về chuẩn tắc (theo định lý giới hạn trung tâm).\n",
    "- Đối với các đặc trưng thuộc loại định tính, sau khi áp dụng `label encoding` để chuyển về dạng số phục vụ cho việc xây dựng mô hình thì nên điều chuẩn để có trung bình và phương sai đồng nhất, giúp cho thuật toán Gradient Descent khi áp dụng sẽ có xu hướng hội tụ nhanh và ổn định."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8da376-d38e-48cc-988c-8ab12d29a062",
   "metadata": {},
   "source": [
    "Bắt đầu điều chuẩn cho đặc trưng `cdi_exp` trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2351b908-7d29-4d2c-b1fe-ba12b2c3430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdi_exp_train_std = standard_scale(cdi_exp_train)\n",
    "cdi_exp_test_std = standard_scale(cdi_exp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abe88fa-815c-48bf-8e86-8b6347d08b65",
   "metadata": {},
   "source": [
    "Tổng hợp lại các đặc trưng đã xử lý chuẩn hoá, điều chuẩn từ trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e6a706-6e0b-4441-bdbe-c4a835cec01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_train = [cdi_train_norm, exp_train_dec, th_train_robust, intensity_train_robust, cdi_exp_train_std]\n",
    "feature_list_test = [cdi_test_norm, exp_test_dec, th_test_robust, intensity_test_robust, cdi_exp_test_std]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26543f23-2ec5-4861-b910-6a666df9a7c6",
   "metadata": {},
   "source": [
    "Tiến hành `label encoding` cho các đặc trưng thuộc loại định tính, sau đó tiến hành điều chuẩn các đặc trưng này"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59d1df50-86b0-4318-8d82-e040d6bf51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['gender', 'relevent_experience', 'enrolled_university', \n",
    "            'education_level', 'major_discipline', 'company_size', \n",
    "            'company_type', 'last_new_job', 'city']\n",
    "\n",
    "for col in cat_cols:\n",
    "    enc_train = label_encode(data_train_filtered, col)\n",
    "    feature_list_train.append(standard_scale(enc_train))\n",
    "    \n",
    "    enc_test = label_encode(data_test, col)\n",
    "    feature_list_test.append(standard_scale(enc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d571dd-de1c-4ff9-9427-c46ad6a12827",
   "metadata": {},
   "source": [
    "Sau đó gộp lại thành 1 dữ liệu hoàn chỉnh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4e01992-02e6-49d2-8f19-b3c60b5f984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = np.column_stack(feature_list_train)\n",
    "X_test_final = np.column_stack(feature_list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d077a-0fb5-48d8-b3d5-5c09c3316501",
   "metadata": {},
   "source": [
    "Bước cuối cùng trong quy trình tiền xử lý sẽ là **giảm chiều dữ liệu** (Dimensionality Reduction) bằng kỹ thuật `PCA` (Principal Components Analysis). Kỹ thuật này đi qua 5 bước xử lý như sau: \n",
    "\n",
    "- **Bước 1: Center data**\n",
    "  \n",
    "    + Công thức: $X_{centered} = X - \\mu$\n",
    "      \n",
    "    + Ý nghĩa: `PCA` hoạt động dựa trên việc tối đa hóa phương sai. Để tính phương sai chính xác, dữ liệu cần được dời về điểm 0 (mean = 0 như đã được chuẩn hoá từ trước) bằng cách lấy giá trị của mỗi đặc trưng trừ đi giá trị trung bình.\n",
    " \n",
    "- **Bước 2: Covariance Matrix (Ma trận hiệp phương sai)**\n",
    "\n",
    "    + Công thức: $\\Sigma = \\frac{1}{N-1} X^T X$\n",
    "  \n",
    "    + Ý nghĩa: Đo lường mối quan hệ tuyến tính giữa các đặc trưng bằng cách sử dụng phép nhân ma trận `np.dot` giữa $X^T$ and $X$, sau đó chia kết quả này cho n_samples - 1 (sử dụng Bessel's correction) để tính phương sai mẫu (sample covariance), giúp cho kết quả không bị biased.\n",
    " \n",
    "- **Bước 3: Eigen Decomposition (Phân rã trị riêng)**\n",
    "\n",
    "    + Ý nghĩa: Tìm ra các hướng (eigenvectors) mà dữ liệu biến thiên nhiều nhất và độ lớn của sự biến thiên đó (eigenvalues) bằng cách dùng `np.linalg.eigh`.\n",
    "- **Bước 4: Sắp xếp và Chọn k thành phần (k = n_components)**\n",
    "\n",
    "    + Ý nghĩa: PCA quan tâm đến các thành phần có phương sai lớn nhất. Vậy nên ta sắp xếp và lấy ra `n_components` đặc trưng đầu tiên tương ứng với k hướng (eigenvectors) quan trọng nhất, chứa nhiều thông tin nhất (k giá trị eigenvalues lớn nhất).\n",
    " \n",
    "- **Bước 5: Transform (Chuyển đổi)**\n",
    "\n",
    "    + Công thức: $Y = X \\cdot W$, trong đó $W$ là ma trận các eigenvector đã chọn ở bước 4. \n",
    "    + Ý nghĩa: Chuyển đổi dữ liệu gốc sang chiều mới (k chiều như đã lấy). Kích thước mới của dữ liệu là $(N \\times D) \\cdot (D \\times K) \\rightarrow (N \\times K)$. Kết quả cho thấy dữ liệu đã giảm chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93bdbb99-7608-4e50-b4e6-50f5fac80c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca, explained_var = perform_pca(X_train_final, n_components=X_train_final.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dfa43-f4cd-45fa-8354-f4f960073c10",
   "metadata": {},
   "source": [
    "Dữ liệu sau khi xử lý thì tiến hành lưu vào thư mục `data/processed` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0ac37a9-95ad-4053-a955-ae7641a08695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu file tại: ../data/processed/aug_train.csv\n",
      "Đã lưu file tại: ../data/processed/aug_test.csv\n"
     ]
    }
   ],
   "source": [
    "processed_train_path = os.path.join('..', 'data', 'processed', 'aug_train.csv')\n",
    "processed_test_path = os.path.join('..', 'data', 'processed', 'aug_test.csv')\n",
    "\n",
    "save_processed_csv(X_train_pca, y_train, processed_train_path)\n",
    "save_processed_csv(X_test_final, None, processed_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff6f72-b214-4a3e-81e3-664749160ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
